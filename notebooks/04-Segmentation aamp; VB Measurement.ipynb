{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Segmentation\n",
    "\n",
    "In the final step, we will perform the segmentation and wear measurement. Your task will be to receive unseen images for the model, perform segmentation on these images, and finally measure the wear thickness to plot the corresponding VB curve for each cutting edge.\n",
    "\n",
    "For having a better results in segmentation, you will receive a weight that has been trained on a larger dataset, resulting in improved accuracy. However, before proceeding with the segmentation task, certain preprocessing steps are required to ensure optimal results.\n",
    "\n",
    "If you have any doubts about the previous section, please go ahead and book a consultation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:18:16.492042: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "model_path = Path(\"..\") / \"model.h5\"\n",
    "\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. Image Alignment\n",
    "Before delving into segmentation, it's essential to address potential issues stemming from image acquisition, such as image shifting. To mitigate these issues and ensure consistency across images, we will employ image alignment techniques.\n",
    "\n",
    "By aligning the images based on a reference or source image, we can ensure that the cutting edge, or the region of interest, remains roughly in the same position across all images. This not only simplifies the segmentation process but also improves the accuracy of subsequent measurements, such as wear thickness calculation.\n",
    "\n",
    "With the images properly aligned, we can proceed confidently to the segmentation phase, where we'll extract meaningful insights regarding wear thickness and plot the corresponding VB curve.\n",
    "\n",
    "You can find the images and the weight here: https://bwsyncandshare.kit.edu/s/kpdaZs6EtcM5Bpn\n",
    "\n",
    "The test dataset comprises a complete set of cutting-edge images, starting from the beginning to the end of the tool's lifespan (approximately 200μm). As the tool has four cutting edges, four images are captured after each data collection step. The collection step occurs after every 10 tests, meaning the milling process is performed 10 times then four images are taken. This sequence is then repeated.\n",
    "\n",
    "***Please refresh the images folder to view the images in the notebook (Download the updated images from ilias)\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Images/image-14.png\" alt=\"Alt text\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Images/image-15.png\" alt=\"Alt text\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TASK: Try using the OpenCV library to implement the alignment method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we believe scaling is not necessary. only trans and rotation. so euclidean motion model is sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 25/275 [05:38<56:26, 13.54s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Estimate warp matrix using ECC\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m     cc, warp_matrix_est \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindTransformECC\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimgRef_gray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgTest_gray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarp_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarp_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriteria\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Warp the image\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     aligned_img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpAffine(\n\u001b[1;32m     49\u001b[0m         imgTest, warp_matrix_est, (sz[\u001b[38;5;241m1\u001b[39m], sz[\u001b[38;5;241m0\u001b[39m]), flags\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_LINEAR \u001b[38;5;241m+\u001b[39m cv2\u001b[38;5;241m.\u001b[39mWARP_INVERSE_MAP\n\u001b[1;32m     50\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import os \n",
    "import numpy as np \n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Folder with the images\n",
    "int_folder = Path(\"..\") / \"test\" / \"Test_Dataset\"\n",
    "\n",
    "# Load image filenames\n",
    "image_files = sorted([filename for filename in os.listdir(int_folder) if filename.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "image_files_source = [image_files[0]]\n",
    "image_files_test = image_files[1:]\n",
    "\n",
    "# Load reference image\n",
    "img_path = os.path.join(int_folder, image_files_source[0])\n",
    "imgRef = cv2.imread(img_path)\n",
    "imgRef_gray = cv2.cvtColor(imgRef, cv2.COLOR_BGR2GRAY)\n",
    "sz = imgRef.shape\n",
    "\n",
    "# Use Euclidean motion model: rotation + translation only\n",
    "warp_mode = cv2.MOTION_EUCLIDEAN\n",
    "\n",
    "# Initialize 2x3 warp matrix to identity\n",
    "warp_matrix = np.eye(2, 3, dtype=np.float32)\n",
    "\n",
    "# ECC optimization parameters\n",
    "number_of_iterations = 1000\n",
    "termination_eps = 1e-6\n",
    "criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, number_of_iterations, termination_eps)\n",
    "\n",
    "# Create output folder\n",
    "aligned_folder = int_folder / \"aligned\"\n",
    "aligned_folder.mkdir(exist_ok=True)\n",
    "\n",
    "for filename in tqdm(image_files_test): \n",
    "    img_path = os.path.join(int_folder, filename)\n",
    "    imgTest = cv2.imread(img_path)\n",
    "    imgTest_gray = cv2.cvtColor(imgTest, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Estimate warp matrix using ECC\n",
    "    try:\n",
    "        cc, warp_matrix_est = cv2.findTransformECC(\n",
    "            imgRef_gray, imgTest_gray, warp_matrix.copy(), warp_mode, criteria\n",
    "        )\n",
    "\n",
    "        # Warp the image\n",
    "        aligned_img = cv2.warpAffine(\n",
    "            imgTest, warp_matrix_est, (sz[1], sz[0]), flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP\n",
    "        )\n",
    "\n",
    "        # Save aligned image\n",
    "        aligned_filename = aligned_folder / f\"aligned_{filename}\"\n",
    "        cv2.imwrite(str(aligned_filename), aligned_img)\n",
    "        # print(f\"Aligned: {aligned_filename}\")\n",
    "    \n",
    "    except cv2.error as e:\n",
    "        print(f\"Failed to align {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. ROI Extraction\n",
    "\n",
    "In our data preprocessing step, we have already identified the regions of interest (ROI) within the original images. Now, our task is to crop these ROIs from the test dataset as well. This is crucial for reducing the size of the images, which can streamline subsequent processing steps and improve computational efficiency.\n",
    "\n",
    "By extracting only the relevant regions from the original images, we focus our computational resources on the areas that are most important for our analysis. This not only speeds up processing but also helps in maintaining the integrity of the data by minimizing irrelevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TASK: Crop the Region of Interest (ROI) area, as performed in the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output directories\n",
    "int_folder = Path(\"..\") / \"test\" / \"Test_Dataset\" / \"aligned\"\n",
    "out_folder = Path(\"..\") / \"test\" / \"Test_Dataset\" / \"cropped\"\n",
    "out_folder.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def crop_imgs(input_dir: Path, output_dir) -> None:\n",
    "   for img_path in tqdm(input_dir.glob(\".jpg\")):\n",
    "      #crop the center section ~> 1400W * 1840H\n",
    "      with Image.open(img_path) as img:\n",
    "         width, height = img.size\n",
    "         left = (width - 1400) // 2\n",
    "         top = (height - 1840) // 2\n",
    "         right = left + 1400\n",
    "         bottom = top + 1840\n",
    "         cropped = img.crop((left, top, right, bottom))\n",
    "         cropped.save(output_dir / img_path.name)\n",
    "   return\n",
    "\n",
    "crop_imgs(int_folder, out_folder)\n",
    "print(\"Cropping completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3. Image resizing\n",
    "\n",
    "In order to utilize the provided segmentation model effectively, you need to resize the images to a specific size, in this case, 512 x 512 pixels. Resizing images to a consistent dimension ensures compatibility with the model's input requirements and helps maintain the aspect ratio of the original images.\n",
    "\n",
    "* TASK: Implement the code to resize the images to the desired dimensions\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Images/image-17.png\" alt=\"Alt text\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TASK: Resize the images to match the dimensions of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# Input and output directories\n",
    "int_folder = Path(\"..\") / \"test\" / \"Test_Dataset\" / \"cropped\"\n",
    "out_folder = Path(\"..\") / \"test\" / \"Test_Dataset\" / \"resized\"\n",
    "out_folder.mkdir(exist_ok=True)\n",
    "\n",
    "def crop_imgs(input_dir: Path, output_dir) -> None:\n",
    "   for img_path in tqdm(input_dir.glob(\".jpg\")):\n",
    "      #crop the center section ~> 1400W * 1840H\n",
    "      with Image.open(img_path) as img:\n",
    "         width, height = img.size\n",
    "         left = (width - 1400) // 2\n",
    "         top = (height - 1840) // 2\n",
    "         right = left + 1400\n",
    "         bottom = top + 1840\n",
    "         cropped = img.crop((left, top, right, bottom))\n",
    "         cropped.save(output_dir / img_path.name)\n",
    "   return\n",
    "        \n",
    "\n",
    "print(\"Resizing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4. Segmenattion Model Implemanation\n",
    "\n",
    "Now that we have completed all the preprocessing steps, our images are prepared for segmentation. By utilizing the provided weight, you can generate the segmentation results for these preprocessed images.\n",
    "\n",
    "ATTENTION: YOU NEED TO RESIZE THE SEGMENTATION RESULT TO THE ORIGINAL SIZE TO KEEP THE PIXEL RATIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TASK: Implement the segmentation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output directories\n",
    "int_folder = \"Input Directory\"\n",
    "out_folder = \"Output Directory\"\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(('.jpg', '.jpeg', '.png')): \n",
    "        ~~~\n",
    "\n",
    "print(\"Prediction completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the segmentation results obtained after applying the segmentation model.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Images/image-19.png\" alt=\"Alt text\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Wear Measurement\n",
    "In this step, we are going to find out the thickness of the wear area in each image, so we can see a trend of how the wear is changing by continuing the milling process. Of course, the segmentation results are not 100% perfect, but as we just need to focus on the maximum thickness in each image, we are on the safe side. At the end of this step, there will be a value for the maximum thickness for each image. The pixel ratio that we have in the image is 1.725 μm per pixel. In this section, you have complete freedom to implement your own solution, however I will provide some hints to get you started. Let's dive into the last step :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. label the Target Cluster\n",
    "\n",
    "If you check the results, you will probably see some misdetections where the model labeled areas as wear, which are not. To eliminate these artifacts and focus on the target wear area, you can implement a clustering method to identify the target area. Since the images are already aligned and the cutting edge coordinates are similar, you can also define a bounding box to locate the target cluster more accurately.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Images/image-20.png\" alt=\"Alt text\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TASK: Try to locate the right segmentation area. For this purpose, you can use findContours from OpenCV, but you can also implement your own solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contour_indices(contours):\n",
    "    valid_indices = []\n",
    "    for i in range(len(contours)):\n",
    "        if len(contours[i]) > 50:\n",
    "            for j in range(len(contours[i])):\n",
    "                    ~~~~\n",
    "                    break  \n",
    "    return valid_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. VB Measuring\n",
    "\n",
    "In this step, we will measure the maximum thickness of the wear. You can implement a combination of functions from OpenCV, such as findContours and Canny edge detection, to find the thickness. It's important to calculate the orthogonal distance (line 1 in Max VB) each time, not the horizontal distance (line 2 in Max VB).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Images/image-21.png\" alt=\"Alt text\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TASK: Try to measure the maximum wear thickness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"Input Directory\"\n",
    "mg_name = []\n",
    "VBmax = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    img_name.append(filename) \n",
    "    img = cv2.imread(os.path.join(directory,filename), cv2.IMREAD_GRAYSCALE)\n",
    "    ret,thresh = cv2.threshold(img,127,255,0)\n",
    "    contours,hierarchy = cv2.findContours(thresh, 1, 2)\n",
    "    result = find_contour_indices(contours)\n",
    "    print(result)\n",
    "    min_max_list = []\n",
    "    \n",
    "    for i in result:\n",
    "        ~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. VB Curve\n",
    "\n",
    "After extracting the maximum thickness of the wear in each image, as already mentioned, the images are for 4 cutting edges, each after 10 cuts. You can plot the VB curve for each cutting edge. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Images/image-22.png\" alt=\"Alt text\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TASK: plot the VB curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB = Workbook() \n",
    "Sheet1 = WB.add_sheet ('VB') \n",
    "\n",
    "Sheet1.write (0,0,,~~) \n",
    "Sheet1.write (1,0,,~~)\n",
    "Sheet1.write (2,0,,~~) \n",
    "Sheet1.write (3,0,,~~)\n",
    "Sheet1.write (4,0,~~) \n",
    "\n",
    "\n",
    "for i in range(len(VBmax_new)): \n",
    "    Sheet1.write (0,i+1,str(i+1) +'_Track',style) \n",
    "    Sheet1.write (1,i+1,float(VBmax_new[i][0]))\n",
    "    Sheet1.write (2,i+1,float(VBmax_new[i][1]))\n",
    "    Sheet1.write (3,i+1,float(VBmax_new[i][2]))\n",
    "    Sheet1.write (4,i+1,float(VBmax_new[i][3])) \n",
    "\n",
    "WB.save (r\"Output Directory\") \n",
    "\n",
    "# Extracting the tooth numbers\n",
    "tooth_numbers = [f'{i}th Tooth' for i in range(1, len(VBmax_new) + 1)]\n",
    "\n",
    "# Creating x-axis labels for all 5 cuts\n",
    "x_labels = [f'{i}' for i in range(1, ~~~,5)]\n",
    "\n",
    "# Transposing VBmax_new for easy plotting\n",
    "vb_values_transposed = list(map(list, zip(*VBmax_new)))\n",
    "\n",
    "# Define marker colors\n",
    "marker_colors = [~~~]\n",
    "\n",
    "# Plotting all 5 cuts for each tooth\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, (vb_values, color) in enumerate(zip(vb_values_transposed, marker_colors)):\n",
    "    ~~~\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4. Data Filtering\n",
    "\n",
    "After plotting the VB curve, you might notice that some results do not follow the trend and are outliers. The most obvious ones are those that are higher than the previous and the next points, which is not possible. So, try to filter the data and eliminate the artifact results.\n",
    "\n",
    "To detect and remove such anomalies, you can apply unsupervised learning techniques like DBSCAN (Density-Based Spatial Clustering of Applications with Noise) or use smoothing methods such as LOWESS (Locally Weighted Scatterplot Smoothing) from the statsmodels.nonparametric module.\n",
    "\n",
    "Below is an example of filtered results that you can use for comparison with your own data. (Please re-dowload the folder 'Images' from ilias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Images/image-25.png\" alt=\"Alt text\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TASK: Implement a clustering method for data filtering and eliminate the outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5. Video Processing\n",
    "\n",
    "In addition to still images, you may also have videos of the cutting tool rotating slowly in front of a camera. In these videos, the tool rotates multiple times, so each cutting edge appears several times across different frames.\n",
    "\n",
    "Your task is to automatically detect the frames corresponding to each cutting edge, starting with identifying the first clear frame for each of the four cutting edges. Then, extract all frames belonging to each respective cutting edge throughout the video. Finally, apply your existing wear characterization algorithm to detect and measure the wear area from the selected frames.\n",
    "\n",
    "Below is a step-by-step guideline to help you structure your solution:\n",
    "\n",
    "* Extract all frames from the video using OpenCV or similar tools.\n",
    "\n",
    "* Identify the first frame where each of the four cutting edges is clearly visible, ideally matching the orientation and appearance of your existing image dataset.\n",
    "\n",
    "* Track and collect all frames that show each respective cutting edge during subsequent tool rotations.\n",
    "\n",
    "* Use techniques such as:\n",
    "\n",
    "    * Edge detection + template matching (cv2.matchTemplate()), or\n",
    "\n",
    "    * Image similarity metrics like SSIM (Structural Similarity Index) or histogram comparison to match frames with known reference images.\n",
    "\n",
    "* Group the detected frames into four categories, each representing one cutting edge.\n",
    "\n",
    "* Apply your wear detection algorithm to the grouped frames to measure and analyze wear over time or rotation.\n",
    "\n",
    "By completing this task, you will automate the analysis of video data, enabling consistent wear monitoring of each cutting edge from continuous tool rotation footage.\n",
    "\n",
    "Link to videos: https://bwsyncandshare.kit.edu/s/kpdaZs6EtcM5Bpn\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Images/image-30.png\" alt=\"Alt text\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kip-2xjLlclH-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
